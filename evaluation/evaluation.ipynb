{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "\n",
    "SELECT_URL = \"http://localhost:5002/solr/articles/select\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS TABLE\n",
    "# Define custom decorator to automatically calculate metric based on key\n",
    "metrics = {}\n",
    "def metric(f): return metrics.setdefault(f.__name__, f)\n",
    "\n",
    "\n",
    "@metric\n",
    "def ap(results, relevant):\n",
    "    \"\"\"Average Precision\"\"\"\n",
    "    precision_values = [\n",
    "        len([\n",
    "            doc\n",
    "            for doc in results[:idx]\n",
    "            if doc in relevant\n",
    "        ]) / idx\n",
    "        for idx in range(1, len(results))\n",
    "    ]\n",
    "    return sum(precision_values)/len(precision_values)\n",
    "\n",
    "\n",
    "@metric\n",
    "def p10(results, relevant, n=10):\n",
    "    \"\"\"Precision at N\"\"\"\n",
    "    return len([doc for doc in results[:n] if doc in relevant])/n\n",
    "\n",
    "@metric\n",
    "def re(results, relevant):\n",
    "    \"\"\"Recall\"\"\"\n",
    "    return len([doc for doc in results if doc in relevant])/len(relevant)\n",
    "\n",
    "\n",
    "def calculate_metric(key, results, relevant):\n",
    "    return metrics[key](results, relevant)\n",
    "\n",
    "\n",
    "# Define metrics to be calculated\n",
    "evaluation_metrics = {\n",
    "    'ap': 'Average Precision',\n",
    "    'p10': 'Precision at 10 (P@10)',\n",
    "    're': 'Recall',\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_articles(articles):\n",
    "    return list(map(lambda x: \"{}/{}/{}\".format(x[\"book\"], x[\"key\"], x[\"date\"]), articles))\n",
    "\n",
    "def metrics_table(search_results, qrels_results):\n",
    "    search_results = _parse_articles(search_results)\n",
    "\n",
    "    # Calculate all metrics and export results as LaTeX table\n",
    "    df = pd.DataFrame([['Metric', 'Value']] +\n",
    "                      [\n",
    "        [evaluation_metrics[m], calculate_metric(\n",
    "            m, search_results, qrels_results)]\n",
    "        for m in evaluation_metrics\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search by articles containing \"horas extraordinárias\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        0         1\n",
      "0                  Metric     Value\n",
      "1       Average Precision  0.600397\n",
      "2  Precision at 10 (P@10)       0.3\n",
      "3                  Recall       1.0\n"
     ]
    }
   ],
   "source": [
    "# you've got to do this manually\n",
    "# for this example I searched for \"horas extraordinárias\" in the regular file\n",
    "# and these IDs are \"book/key/date\"\n",
    "qrels = [\n",
    "    \"Código Penal/58/1995-03-15T00:00:00Z\",\n",
    "    \"Código Penal/58/2017-11-21T00:00:00Z\",\n",
    "    \"Código Penal/58/2007-09-15T00:00:00Z\",\n",
    "]\n",
    "\n",
    "results = requests.get(\n",
    "    SELECT_URL,\n",
    "    params={\n",
    "        \"defType\": \"edismax\",\n",
    "        \"qf\": \"title_raw^3 title^2 text_raw^1.5 text^1 path^0.5 book^0.25\",\n",
    "        \"bq\": \"state:Consolidado^4\",\n",
    "        \"pf\": \"title_raw^3 title^2 text_raw^1.5 text^1 path^0.5 book^0.25\",\n",
    "        \"q\": \"horas extraordinárias\"\n",
    "    }\n",
    ").json()['response']['docs']\n",
    "\n",
    "# average precision takes the order into account\n",
    "# precision at 10 is the percentage of relevant information in the top10\n",
    "# recall is the percentage of relevant information in the total of results\n",
    "metrics_table(results, qrels)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
